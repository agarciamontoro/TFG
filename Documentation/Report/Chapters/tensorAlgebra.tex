\chapter{Tensor Algebra}

This chapter covers some basic tools needed in the further development of this work.

\section{Tensors on a generic vector space}

\subsection{The notion of tensor}

\begin{definition}[Multilinear map]
	\label{def:multilinear}
    Let $V_1, V_2, \dots, V_r$ and $W$ be vector spaces over the same field $K$. A multilinear ---$r$ times linear--- map from $V_1 \times V_2 \cdots \times V_r$ to $W$ is a map
    \[
        T \colon V_1 \times V_2 \cdots \times V_r \longrightarrow W
    \]
    that is linear in each of its components; \ie, that verifies the following conditions:
    \begin{enumerate}
        \item $\begin{aligned}[t]
	        T(x_1, \dots, x_i+x_i', \dots, x_r) = &T(x_1, \dots, x_i, \dots, x_r) + \\&T(x_1, \dots, x_i', \dots, x_r),
        \end{aligned}$
        \item $T(x_1, \dots, a x_i, \dots, x_r) = aT(x_1, \dots, x_i, \dots, x_r)$,
    \end{enumerate}
    for every $i \in \{1, 2, \dots, r\}$, where $x_j$ is an arbitrary vector in $V_j$ and $a \in K$.
\end{definition}

Before going ahead with the definition of tensor, we must remember the concept of dual space.

Given a vector space $V$ over a field $K$, its \emph{dual space} is the vector space defined as
\[
	V^* \defeq \Hom_K(V,K);
\]
that is, $V^*$ is the set of all linear maps $\varphi : V \to K$.

There are some interesting results concerning dual spaces that will be important in the understanding of the notion of tensor.

First of all, it is known that if $V$ is finite-dimensional, the dimensions of $V$ and $V^*$ are the same and, given a base of $V$, $B = \{v_1, \dots, v_n\}$, its \emph{dual basis} is built as $B^* = \{\varphi^1, \dots, \varphi^n\}$\footnote{From now on, Latin letters with subscripts will denote vectors, whereas Greek letters with superscripts will denote one-forms.}, where
\[
	\varphi^i(v_j) = \delta^i_j.
\]

Furthermore, the reflexiveness theorem \change{Reflexiveness theorem is the worst translation ever} tells us that there exists a natural isomorphism between $V$ and its double dual space, $V^{**}$, when $V$ is finite-dimensional. This isomorphism assigns, to every vector $v \in V$, a function that maps every one-form into its evaluation on $v$:
\begin{align*}
	\psi \colon V &\longrightarrow V^{**} \\
	v &\longmapsto \psi_v \colon \begin{aligned}[t]
		V^* &\longrightarrow K \\
		\varphi &\longmapsto \varphi(v).
	\end{aligned}
\end{align*}

With the concepts of multilinear maps and dual spaces we can now build the definition of tensor, core concept of this section.

\begin{definition}[Tensor]
	\label{def:tensor}
	Let $V$ be a vector space over a field $K$, being $V^*$ its dual space. A tensor $r$ ($\geq 0$) times contravariant and $s$ ($\geq 0$) times covariant ---\ie, a tensor of type $(r,s)$--- is a multilinear map
	\[
		T \colon \underbrace{V^* \times \cdots \times V^*}_{\text{r copies}} \times \underbrace{V \times \cdots \times V}_{\text{s copies}} \longrightarrow K.
	\]
	Tensors of type $(0,s)$ are said to be \emph{covariant}, whereas tensors of type $(r,0)$ are called \emph{contravariant}.
	
	The \emph{order} of the tensor is defined as the sum $r+s$.
\end{definition}

\begin{example}
	\label{ex:tensors}
	The following examples show how interesting the notion of tensor is, as it can include a vast selection of mathematical objects under the same concept; for example, we will see that both vectors and one-forms are tensors.
	
	Let $V$ be an $n$-dimensional vector space and $V^*$ its dual space.
	\begin{enumerate}
		\item Let $\varphi \in V^*$; \ie,  $\varphi \colon V \to K$ is a one-form. From \autoref{def:tensor} it is clear that $\varphi$ is a tensor of type $(0,1)$ over $V$.
		\item Let $v \in V$ be a vector. Using the natural isomorphism between $V$ and its double dual space, the vector $v$ can be identified with $\psi_v \colon V^* \to K$, and thus we can understand $v$ as a tensor of type $(1,0)$.
		\item Consider now $f \in \End_K V$ and let $T_f \colon V^* \times V \to K$ be the map defined as $T_f(v, \varphi) \defeq \varphi(f(v))$. It is clear that $T_f$ is a tensor of type $(1,1)$. Moreover, if we consider $f$ to be the identity map $1_V$, then $T_{1_V}$ is the tensor that maps every pair of (vector, one-form) to the evaluation of the one-form on the vector; \ie, is the tensor associated to the natural isomorphism between $V$ and $V^{**}$.
		\item Common operations on several mathematical fields can also be seen as tensors. For example, the inner product can be defined as a tensor of type $(0,2)$ as follows:
		\begin{align*}
			T \colon V \times V &\to K\\
			(v,w) &\mapsto \sum_{i=1}^n v_i w_i.
		\end{align*}
		In general, every bilinear form is a $(0,2)$ tensor. This follows from the definition of bilinear form, which satisfies all conditions in \autoref{def:tensor}.
	\end{enumerate}
\end{example}

\subsection{Tensor addition, product by a scalar and tensor product}
Let $\tensors_{r,s}(V)$ be the set of all tensors of type $(r,s)$. It is clear that both $\tensors_{1,0}(V) = V^*$ and $\tensors_{0,1}(V) = V^{**}$ are vector spaces over K. A natural question arises: is $\tensors_{r,s(V)}$ a vector space for arbitrary $r$ and $s$? The following result gives us the answer we are looking for.

\begin{theorem}
	Let $T, T' \in \tensors_{r,s}(V)$ be two tensors of type $(r,s)$ and $a \in K$ a scalar. Consider the following operations:
	\begin{itemize}
		\item $\begin{aligned}[t]
			(T+T') (\varphi^1, \dots, \varphi^r, v_1, \dots, v_s) \defeq &T(\varphi^1, \dots, \varphi^r, v_1, \dots, v_s) +\\
			&T'(\varphi^1, \dots, \varphi^r, v_1, \dots, v_s),
		\end{aligned}$
		\item $(a T)(\varphi^1, \dots, \varphi^r, v_1, \dots, v_s) \defeq a T(\varphi^1, \dots, \varphi^r, v_1, \dots, v_s)$,
	\end{itemize}
	where $\varphi^i \in V^*$ for all $i \in \{1,\dots,r\}$ and $v_j \in V$ for all $j \in \{1,\dots,s\}$.
	
	The set $\tensors_{r,s}(V)$ with the preceding operations is a vector space.
\end{theorem}

\begin{proof}
	It is clear, from \autoref{def:tensor}, that $T+T', aT \in \tensors_{r,s}(V)$, given the linearity in each of the components of both $T$ and $T'$.
	These operations satisfy the following properties:
	\begin{enumerate}
		\item $(T+T') + T'' = T + (T'+T'')$.
		\item There exists a \emph{null tensor} $T_0 \in \tensors_{r,s}(V)$, defined as \[T_0(\varphi^1, \dots, \varphi^r, v_1, \dots, v_s) = 0, \forall \varphi^i \in V^*, \forall v_i \in V,\] such that $T_0 + T = T + T_0 = T \;\forall T \in \tensors_{r,s}(V)$.
		\item For each $T\in\tensors_{r,s}(V)$ there exists an \emph{opposite tensor} $-T$ defined as \[(-T)(\varphi^1, \dots, \varphi^r, v_1, \dots, v_s) = - T(\varphi^1, \dots, \varphi^r, v_1, \dots, v_s)\] that satisfies $T + (-T) = -T + T = 0$.
		\item $T + T' = T' + T$.
	\end{enumerate}
	This provides abelian group structure to $\tensors_{r,s}(V)$. The following properties finally show that $\tensors_{r,s}(V)$ is a vector space:
	\begin{enumerate}
		\item $a(T+T') = aT+ aT', \;\;\forall a \in K, \;\forall T,T' \in \tensors_{r,s}(V)$.
		\item $(a+b)T = aT + bT, \;\;\forall a,b \in K, \;\forall T \in \tensors_{r,s}(V)$.
		\item $(ab)T = a(bT), \;\;\forall a,b \in K, \;\forall T \in \tensors_{r,s}(V)$.
		\item $1T = T, \;\;\forall T \in \tensors_{r,s}(V)$, where $1$ is the unity in $K$.
	\end{enumerate}
\end{proof}

Now that we know that $\tensors_{r,s}(V)$ is a vector space, it will be important to study its dimension. Before going down that road, let us define the tensor product, concept upon which we will be able to build a basis for $\tensors_{r,s}(V)$.

\begin{definition}[Tensor product]
	Let $T \in \tensors_{r,s}(V)$ and $T' \in \tensors_{r',s'}(V)$. The \emph{tensor product} \[T \otimes T' \colon \underbrace{V^* \times \cdots \times V^*}_{\text{r+r' copies}} \times \underbrace{V \times \cdots \times V}_{\text{s+s' copies}}\] is defined as follows:
	\begin{align*}
		&(T \otimes T')(\varphi^1, \dots, \varphi^{r+r'}, v_1, \dots, v_{s+s'}) \defeq \\
		&T(\varphi^1, \dots, \varphi^r, v_1, \dots, v_s) T(\varphi^{r+1}, \dots, \varphi^{r+r'}, v_{s+1}, \dots, v_{s+s'}),
	\end{align*}
	where $\varphi^i \in V^*$ for all $i \in \{1,\dots,r+r'\}$ and $v_j \in V$ for all $j \in \{1,\dots,s+s'\}$.
\end{definition}

It is easy to prove that $T \otimes T' \in \tensors_{r+r',s+s'}(V)$ for every $T \in \tensors_{r,s}(V)$ and $T' \in \tensors_{r',s'}(V)$. However, the proof is long and cumbersome to write, and it can be found in almost every elementary book on tensor algebra.\unsure{Add reference.}

Furthermore, we can see that, given $T \in \tensors_{r,s}(V)$, $T' \in \tensors_{r',s'}(V)$ and $T'' \in \tensors_{r'',s''}(V)$:
\begin{align*}
	(T \otimes T') \otimes T'' &\in \tensors_{r+r'+r'', s+s'+s''}(V),\\
	T \otimes (T' \otimes T'') &\in \tensors_{r+r'+r'', s+s'+s''}(V)
\end{align*}
and that the following equality holds:
\[
	(T \otimes T') \otimes T'' = T \otimes (T' \otimes T'').
\]
In fact, the application
\begin{align*}
	\tensors_{r,s}(V) \times \tensors_{r',s'}(V) &\longrightarrow \tensors_{r+r',s+s'}(V) \\
	(T,T') &\longmapsto T \otimes T'
\end{align*}
is a bilinear map.

As a particular example, we can define the tensor product between two tensors of type $(1,0)$ and $(0,1)$ as the following tensor:\footnote{Note that we are using the reflexiveness theorem to ease the notation, as we are writing $v$ ---a letter used for vectors of $V$--- to describe an element of $V^{**}$.}
\[
	v \otimes \varphi \in \tensors_{1,1}(V),
\]
which maps every pair $(\psi, w)$ to the scalar $\psi(v)\varphi(w)$.

Upon this concept, and using the known properties of the dual vector space and its basis, we can state and prove a theorem that builds a basis for every $\tensors_{r,s}(V)$.

\begin{theorem}
	Let $V$ be an $n$-dimensional vector space over a field $K$. Let $\mathcal{B} = \{v_1, \dots, v_n\}$ be a basis of $V$ and $\mathcal{B^*} = \{\varphi^1, \dots, \varphi^n\}$ be  its dual basis. Then,
	\begin{align*}
		\mathcal{B}_T = \{v_{i_1} \otimes \dots \otimes v_{i_r} \otimes \varphi^{j_1} \otimes \dots \otimes \varphi^{j_s} \textrm{ , } &\textrm{where every index moves} \\
		&\textrm{independently from $1$ to $n$}\},
	\end{align*}
	is a basis of $\tensors_{r,s}(V)$. As a consequence, $\dim_K\tensors_{r,s}(V) = n^{r+s}$.
\end{theorem}

\begin{proof}
	%First of all, let us note $t^{i_1,\dots,i_r}_{j_1,\dots,j_s} \defeq T(\varphi^{i_1}, \dots, \varphi^{i_r}, v_{j_1}, \dots, v_{j_r}) \in K$.

	In order to prove that $\mathcal{B}_T$ is a linear span of $\tensors_{r,s}(V)$, let us consider $T\in\tensors_{r,s}(V)$. Then, if
	\begin{align*}
		\psi^1& = \sum_{i_1 = 1}^n a_{i_1}^1 \varphi^{i_1},\quad \cdots,\quad \psi^r = \sum_{i_r = 1}^n a_{i_r}^r \varphi^{i_r} \textrm{ and} \\
		w_1 &= \sum_{j_1 = 1}^n b^{j_1}_1 v_{j_1},\quad \cdots,\quad w_s = \sum_{j_s = 1}^n b^{j_s}_s v_{j_s},
	\end{align*}
	we can write
	\[
		T(\psi^1, \dots, \psi^r, w_1, \dots, w_s) = \sum_{\substack{i_1,\dots,i_s\\j_1,\dots,j_r}} a_{i_1}^1 \cdots a_{i_r}^r b^{j_1}_1 \dots b^{j_s}_s t_{j_1, \dots, j_s}^{i_1, \dots, i_r},
	\]
	where $t_{j_1, \dots, j_s}^{i_1, \dots, i_r} \defeq T(\varphi^{i_1}, \dots, \varphi^{i_r}, v_{j_1}, \dots, v_{j_s})$.
	
	Following this reasoning ---see \cite{romero86} for the details---, we can finally write
	\[
		T = \sum_{\substack{i_1,\dots,i_s\\j_1,\dots,j_r}} t_{j_1, \dots, j_s}^{i_1, \dots, i_r} \left( v_{i_1} \otimes \dots \otimes v_{i_r} \otimes \varphi^{j_1} \otimes \dots \otimes \varphi^{j_s} \right),
	\]
	which proves that $\mathcal{B}_T$ spans $\tensors_{r,s}(V)$.
	
	To prove that $\mathcal{B}_T$ is linearly independent, consider a linear combination of the elements in $\mathcal{B}_T$ that equals the null tensor,
	\begin{equation}
		\label{eq:nullcomb}
		\sum_{\substack{k_1,\dots,k_r\\l_1,\dots,l_s}} a^{k_1,\dots,k_r}_{l_1,\dots,l_s} \left( v_{k_1} \otimes \dots \otimes v_{k_r} \otimes \varphi^{l_1} \otimes \dots \otimes \varphi^{l_s} \right) = T_0
	\end{equation}
	Evaluating both sides of \autoref{eq:nullcomb} on the corresponding elements of the bases $\mathcal{B}$ and $\mathcal{B}^*$ we obtain the equality
	\[
		\sum_{\substack{k_1,\dots,k_r\\l_1,\dots,l_s}} a^{k_1,\dots,k_r}_{l_1,\dots,l_s} \left( \delta^{i_1}_{k_1} \dots \delta^{i_r}_{k_r} \delta^{l_1}_{j_1} \dots \delta^{l_s}_{j_s} \right) = 0,
	\]
	from which it is clear that $a^{i_1,\dots,i_r}_{j_1,\dots,j_s} = 0$ for every index.
\end{proof}

Two particularly important examples of the above theorem let us grasp the duality between vectors and one-forms.

Consider $V$ an $n$-dimensional vector space, with basis $B = \{v_1, \dots, v_n\}$ and dual basis $B^* = \{\varphi^i, \dots, \varphi^n\}$.

We know that $\tensors_{1,0}(V) = V^{**}$, and using reflexiveness theorem we can write each vector $v \in V \simeq V^{**}$ as
\[
v = \sum_{i=1}^n b^i v_i,
\]
where each coordinate is the projection of the vector using the corresponding one-form: $b^i = \varphi^i(v)$.

Similarly, we can consider $\tensors_{0,1}(V) = V^*$. Each of its elements, $\psi \in V^*$, can be written in terms of the tensor basis $\mathcal{B}_{\tensors_{0,1}(V)} = B^*$:
\[
	\psi = \sum_{j=1}^n a_j \varphi^j,
\]
Note here that the coordinates are retrieved evaluating the form in each element of the basis $B$: $a_j = \psi(v_j)$.

This shows that both evaluating a one-form on the elements of the basis $B$ and projecting a vector with the elements of the basis $B^*$ can be seen as dual operations: both of them gives the coordinates of the corresponding element on the corresponding basis.

The notion of tensor help us to understand this duality: we now see both elements as the \emph{same} mathematical entity, where both operations are expressed in different bases with the corresponding change of coordinates.

\subsection{Change of bases on $\tensors_{r,s}(V)$}

We want now to study how the coordinates of a tensor change when we change the basis of $\tensors_{r,s}(V)$.

First of all, we must remember the relationship between the change of basis on a vector space $V$ and the corresponding change of basis in its dual space, $V^*$.

Let $B = (v_1, \dots, v_n)$ and $B' = (v_1', \dots, v_n')$ be two bases of $V(K)$, and suppose that
\begin{equation}
	\label{eq:changeV}
	v_j' = \sum_{i=1}^n b_j^i v_i \textrm{, where } b_j^i \in K.
\end{equation}

It is known that ---see \cite[p. 162]{romero86}---, if $B^* = (\varphi^1, \dots, \varphi^n)$ and $B'^* = (\varphi'^1, \dots, \varphi'^n)$ are the dual bases of the two preceding ones, the elements of the bases are related in the \emph{opposite} way as before:
\[
	\varphi^i = \sum_{j=1}^n b_j^i \varphi'^j.
\]

Furthermore, we can see that
\begin{equation}
	\label{eq:changeV*}
	\varphi'^i = \sum_{l=1}^n a_l^i \varphi'^l,
\end{equation}
where $a_l^i$ is the element placed on the $i$-th row, $l$-th column of  the inverse matrix of $(b^i_j)$.

This change of bases in $V$ and $V^*$  is key to understand \autoref{pro:changeT}, which tells us how  the coordinates of a tensor change when the basis of $\tensors_{r,s}(V)$.

\begin{proposition}
	\label{pro:changeT}
	Let $B$, $B'$ and $B^*$, $B'^*$ the bases of $V$ and $V^*$ described above and consider two ordered bases of $\tensors_{r,s}(V)$ obtained from
	\begin{align*}
		\mathcal{B}_T = \{v_{i_1} \otimes \dots \otimes v_{i_r} \otimes \varphi^{j_1} \otimes \dots \otimes \varphi^{j_s} \textrm{ , } &\textrm{where every index moves} \\
		&\textrm{independently from $1$ to $n$}  \}
	\end{align*}
	and
	\begin{align*}
		\mathcal{B}'_T = \{v'_{i_1} \otimes \dots \otimes v'_{i_r} \otimes \varphi'^{j_1} \otimes \dots \otimes \varphi'^{j_s} \textrm{ , } &\textrm{where every index moves} \\
		&\textrm{independently from $1$ to $n$}  \}.
	\end{align*}
	
	Then, the analytic expression of the change of coordinates of a tensor $T \in \tensors_{r,s}(V)$ is the following:
	\begin{align*}
		t'^{k_1,\dots,k_r}_{\phantom{'}l_1,\dots,l_s} = \sum a_{i_1}^{k_1} \cdots a_{i_k}^{k_r} b_{l_1}^{j_1} \cdots b_{l_s}^{j_s} t_{j_1,\dots,j_s}^{i_1,\dots,i_r}
	\end{align*}
	where the coefficients are the ones defined in \autoref{eq:changeV} and \autoref{eq:changeV*}
\end{proposition}

\begin{proof}
	Considering the change of bases in $V$ and $V^*$, the proof is direct, using the definition of the coordinates of a tensor $T \in \tensors_{r,s}(V)$:
	\begin{align*}
		t'^{k_1,\dots,k_r}_{\phantom{'}l_1,\dots,l_s} &= T(\varphi'^{k_1}, \dots, \varphi'^{k_r}, v_{l_1}', \dots, v_{l_s}')  = \\
		&= T(\sum a_{i_1}^{k_1}\varphi^{i_1}, \dots, \sum a_{i_r}^{k_r}\varphi{i_r}, \sum b_{l_1}^{j_1} v_{j_1}, \dots, \sum b_{l_s}^{j_s} v_{j_s}) = \\
		&= \sum a_{i_1}^{k_1} \cdots a_{i_k}^{k_r} b_{l_1}^{j_1} \cdots b_{l_s}^{j_s} T(\varphi^{k_1}, \dots, \varphi^{k_r}, v_{l_1}, \dots, v_{l_s}) = \\
		&= \sum a_{i_1}^{k_1} \cdots a_{i_k}^{k_r} b_{l_1}^{j_1} \cdots b_{l_s}^{j_s} t_{j_1,\dots,j_s}^{i_1,\dots,i_r}
	\end{align*}
\end{proof}

We can study now the particular examples of tensors of order 2.

For $r = s = 1$, the expression of the change of coordinates is
\[
	t'^k_{\phantom{'}l} = \sum_{i,j = 1}^n a^k_i b_l^j  t^i_j \quad \forall k,l \in \{1,2,\dots,n\}.
\]

This expression can be written using matrix notation as follows:
\begin{equation}
	\label{eq:tensors11}
	(t'^k_{\phantom{'}l}) = \mathbf{A} (t^i_j) \mathbf{A}^{-1}
\end{equation}
where $\mathbf{A}$ is the matrix $\left(a_j^i\right)$ whose elements are the ones defined in \autoref{eq:changeV*}.

This last expression is very interesting, as it tells us that the \emph{matrices} of the coordinates of $T$ in both bases of $\tensors_{r,s}(V)$ are similar.

Remember now the third item in \autoref{ex:tensors}: it established a relationship between $\tensors_{1,1}(V)$ and $\End_K V$, that is now clear again: we see that \autoref{eq:tensors11} is not only the change of coordinates of a tensor, but also the expression of the change of basis matrix of the corresponding endomorphism.

This is not surprising as both spaces are isomorphic, with a \emph{natural} isomorphism between them:
\begin{theorem}
	The map
	\begin{align*}
		\End_K V &\longrightarrow \tensors_{1,1}(V),\\
		f &\longmapsto T_f
	\end{align*}
	where $T_f(\varphi, v) = \varphi(f(v)) \; \forall \varphi \in V^* \; \forall v \in V$, is an isomorphism.
\end{theorem}
\begin{proof}
	It is obvious to see that $T_{f+f'} = T_f +T_{f'}$ and that $T_{af} = aT_f$, which tells us that the map is linear.
	
	Suppose now that $T_f = T_0$; \ie, $\varphi(f(v)) = 0 \; \forall \varphi \in V^* \; \forall v \in V$. This implies immediately that $f(v) = 0 \; \forall v \in V$, as it is known that if $\psi(w) = 0$ for every $\psi \in V$, then $w = 0$. This shows that $f$ is the zero mapping, which proves that the map is one-to-one.
\end{proof}

Similarly, we can study tensors of type $(2,0)$ and $(0,2)$. For tensors of type $(2,0)$, the expression is
\[
	t'^{kl} = \sum_{i,j=1}^n a_i^k a_j^l t^{ij},
\]
whereas for tensors of type $(0,2)$, the change of coordinates is given by
\[
	t'_{kl} = \sum_{i,j=1}^n b_k^i b_l^j t_{ij}.
\]

These expressions can also be written with matrix notation, where $\mathbf{A} = \left(a^i_j\right)$ is the matrix defined above and where we note $\mathbf{A}^{-t}$ to be the transpose of the inverse of $\mathbf{A}$:
\begin{align*}
	\left(t'^{kl}\right) &= \mathbf{A} t^{ij} \mathbf{A}^t \\
	\left(t'_{kl}\right) &= \mathbf{A}^{-t} t_{ij} \mathbf{A}^{-1}
\end{align*}

\subsection{Alternative definition of tensor: the physics approach}








